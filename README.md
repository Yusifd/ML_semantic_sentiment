# ML_semantic_sentiment
# ğŸ“Š Sentiment Analysis on Azerbaijani Sentences

In this project, I experimented with **multilingual sentiment analysis** using two different transformer-based models:

- ğŸŸ¢ **BERT-base multilingual uncased sentiment** (`nlptown/bert-base-multilingual-uncased-sentiment`)
- ğŸ”µ **XLM-Roberta-base sentiment** (`cardiffnlp/twitter-xlm-roberta-base-sentiment`)

---

## âš”ï¸ Model Comparison

To evaluate performance, I tested both models on **100 Azerbaijani sentences** across five sentiment categories:
- Ã‡ox mÉ™nfi (Very Negative)
- MÉ™nfi (Negative)
- Neytral (Neutral)
- MÃ¼sbÉ™t (Positive)
- Ã‡ox mÃ¼sbÉ™t (Very Positive)

### âœ… Results:
- **XLM-Roberta** achieved **accuracy between 79% â€“ 91%**  
- **BERT-base multilingual** achieved **accuracy between 51% â€“ 78%**

ğŸ‘‰ This shows that **XLM-Roberta** is more robust and reliable for Azerbaijani text sentiment classification compared to the multilingual BERT baseline.

---

## ğŸ”¥ Key Highlights
- Built embeddings with `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` for semantic similarity search.
- Generated **heatmaps** with `seaborn` to visualize sentiment probabilities across sentences.
- Grouped predictions into **negative, neutral, positive** categories for easier interpretation.
- Compared **fine-grained sentiment labels** (1â˜… to 5â˜…) vs. **3-class sentiment labels** (Negative, Neutral, Positive).

---

## ğŸ“Š Model Performance Comparison

| Model | Accuracy Range | Strengths | Weaknesses |
|-------|----------------|-----------|------------|
| ğŸ”µ **XLM-Roberta-base sentiment** | **79% â€“ 91%** | Handles Azerbaijani text very well, robust across categories, consistent predictions | Slightly heavier model, requires more resources |
| ğŸŸ¢ **BERT-base multilingual uncased sentiment** | **51% â€“ 78%** | Lightweight, faster inference, decent for general multilingual tasks | Struggles with nuanced Azerbaijani expressions, less accurate and less consistent |

---

### ğŸ“ Key Takeaway
- **XLM-Roberta** clearly outperforms **BERT-base multilingual** for **Azerbaijani sentiment analysis**.  
- If accuracy is the priority â†’ go with **XLM-Roberta**.  
- If speed and lightweight inference matter more â†’ **BERT-base multilingual** can still be used, but expect weaker results.

---


## ğŸš€ Conclusion
- **XLM-Roberta** is the better choice for **Azerbaijani sentiment analysis**, offering higher accuracy and more consistent predictions.  
- **BERT-base multilingual** works but struggles with nuanced Azerbaijani expressions.  
- Future work: fine-tuning XLM-Roberta on a larger Azerbaijani dataset for even better performance.

---

âœ¨ With this comparison, I demonstrated how **model choice** significantly impacts performance in multilingual NLP tasks.  
